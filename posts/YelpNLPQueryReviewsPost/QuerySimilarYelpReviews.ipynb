{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "QuerySimilarYelpReviews.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CH-LzqiciQDl",
        "outputId": "d2c30731-4f6b-496c-c712-90597b3bfe7c"
      },
      "source": [
        "# installs.\n",
        "!python -m spacy download en_core_web_lg # largest english model from spacy"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting en_core_web_lg==2.2.5\n",
            "\u001b[?25l  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-2.2.5/en_core_web_lg-2.2.5.tar.gz (827.9MB)\n",
            "\u001b[K     |████████████████████████████████| 827.9MB 1.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.6/dist-packages (from en_core_web_lg==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (3.0.5)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (2.0.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (51.1.1)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.19.5)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (4.41.1)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (0.8.0)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_lg==2.2.5) (3.3.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_lg==2.2.5) (3.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_lg==2.2.5) (3.7.4.3)\n",
            "Building wheels for collected packages: en-core-web-lg\n",
            "  Building wheel for en-core-web-lg (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for en-core-web-lg: filename=en_core_web_lg-2.2.5-cp36-none-any.whl size=829180945 sha256=f9b4e439ac8ae940af6b40dd00d143302947991aa2969ac6bf5cad0f594630d8\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-s0zz9tfz/wheels/2a/c1/a6/fc7a877b1efca9bc6a089d6f506f16d3868408f9ff89f8dbfc\n",
            "Successfully built en-core-web-lg\n",
            "Installing collected packages: en-core-web-lg\n",
            "Successfully installed en-core-web-lg-2.2.5\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_lg')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3KfqNk7JiJCZ"
      },
      "source": [
        "# imports.\n",
        "import pandas as pd\n",
        "import re\n",
        "import spacy \n",
        "from spacy.tokenizer import Tokenizer\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import GridSearchCV"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 217
        },
        "id": "_LY8P5wRiM-j",
        "outputId": "a468ae7c-7188-4347-83c8-c76e5c9aee74"
      },
      "source": [
        "# read in the Yelp json file \n",
        "yelp = pd.read_json('https://raw.githubusercontent.com/CVanchieri/LambdaSchool-Sprints/master/Sprints/DS/Unit4/Sprint11_CharlesVanchieri/review_sample.json?token=AHYSXEC7KWZRBP6A5YGAUOTABNTOM', lines=True)\n",
        "yelp = yelp[['business_id', 'review_id', 'text', 'cool', 'funny', 'useful', 'stars']]\n",
        "print(yelp.shape)\n",
        "yelp.head()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(10000, 7)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>business_id</th>\n",
              "      <th>review_id</th>\n",
              "      <th>text</th>\n",
              "      <th>cool</th>\n",
              "      <th>funny</th>\n",
              "      <th>useful</th>\n",
              "      <th>stars</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>nDuEqIyRc8YKS1q1fX0CZg</td>\n",
              "      <td>eZs2tpEJtXPwawvHnHZIgQ</td>\n",
              "      <td>BEWARE!!! FAKE, FAKE, FAKE....We also own a sm...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>10</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>eMYeEapscbKNqUDCx705hg</td>\n",
              "      <td>DoQDWJsNbU0KL1O29l_Xug</td>\n",
              "      <td>Came here for lunch Togo. Service was quick. S...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>6Q7-wkCPc1KF75jZLOTcMw</td>\n",
              "      <td>DDOdGU7zh56yQHmUnL1idQ</td>\n",
              "      <td>I've been to Vegas dozens of times and had nev...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>k3zrItO4l9hwfLRwHBDc9w</td>\n",
              "      <td>LfTMUWnfGFMOfOIyJcwLVA</td>\n",
              "      <td>We went here on a night where they closed off ...</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>6hpfRwGlOzbNv7k5eP9rsQ</td>\n",
              "      <td>zJSUdI7bJ8PNJAg4lnl_Gg</td>\n",
              "      <td>3.5 to 4 stars\\n\\nNot bad for the price, $12.9...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "              business_id               review_id  ... useful  stars\n",
              "0  nDuEqIyRc8YKS1q1fX0CZg  eZs2tpEJtXPwawvHnHZIgQ  ...     10      1\n",
              "1  eMYeEapscbKNqUDCx705hg  DoQDWJsNbU0KL1O29l_Xug  ...      0      4\n",
              "2  6Q7-wkCPc1KF75jZLOTcMw  DDOdGU7zh56yQHmUnL1idQ  ...      2      3\n",
              "3  k3zrItO4l9hwfLRwHBDc9w  LfTMUWnfGFMOfOIyJcwLVA  ...      5      1\n",
              "4  6hpfRwGlOzbNv7k5eP9rsQ  zJSUdI7bJ8PNJAg4lnl_Gg  ...      5      4\n",
              "\n",
              "[5 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t6Pfiz4NiNBM"
      },
      "source": [
        "yelp['text'] = yelp['text'].apply(lambda x: re.sub(r'[^a-zA-Z ^0-9]', '', x)) # keep only letters and numbers.\n",
        "yelp['text'] = yelp['text'].apply(lambda x: re.sub(r'(x.[0-9])', '', x)) # remove any special characters. \n",
        "yelp['text'] = yelp['text'].replace('/', ' ') # remove additional white spaces.\n",
        "yelp['text'] = yelp['text'].apply(lambda x: re.sub('  ', ' ', x)) # convert  the text to lowercase.\n",
        "yelp['text'] = yelp['text'].apply(lambda x: x.lower())"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "id": "vvU7_iJpiNEL",
        "outputId": "f6d93555-3f5e-4ecb-f313-86630de55116"
      },
      "source": [
        "yelp.text[4] # example reviews text "
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'35 to 4 starsnot bad for the price 1299 for lunch seniors get 15 off pay at the front before eating there are hot food salads noodle bar dessert fruits fried varieties and soupseating in the middle section is a bit too tight but the booths on the side look more spaciousi think the lunch noodle bar would have more variety such as different types of greens vermicelli noodles frozen tofu etc the tomato and laksa broth were both pretty good'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qpSkLdHziNJb"
      },
      "source": [
        "df = yelp.copy() # copy the dataframe."
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ll38Vsudid-a"
      },
      "source": [
        "nlp = spacy.load(\"en_core_web_lg\") # set the nlp and load the \"en_core_web_lg\" file.\n",
        "tokenizer = Tokenizer(nlp.vocab) # use to tokenizer on the nlp.vocab\n",
        "STOP_WORDS = nlp.Defaults.stop_words # use the nlp default stop words"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SqDBv6NVijcZ",
        "outputId": "47cae185-4623-45d7-b8bb-43450a7a4000"
      },
      "source": [
        "### tokenizer pipeline, remove stop words, blank words, and lemmatize ###\n",
        "tokens = [] # create a list for the tokens \n",
        "for doc in tokenizer.pipe(df['text'], batch_size=500): # run the df['text] data through the tokenizer in batch sizes, loop through each doc\n",
        "    doc_tokens = [] # create a list for the doc tokens\n",
        "    for token in doc: # loop through each token in the doc\n",
        "        if (token.lemma_ not in STOP_WORDS) & (token.text != ' '): # if the token.lemma_ is not in the stop words and token.text is not blank space\n",
        "            doc_tokens.append(token.lemma_) # add the token.lemma_ to the doc tokens list \n",
        "    tokens.append(doc_tokens) # add the doc tokens list to the tokens list \n",
        "# set the new tokens in the data frame.\n",
        "df['tokens'] = tokens # add a new new tokens list to a new column \n",
        "df['tokens']"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0       [beware, fake, fake, fakewe, small, business, ...\n",
              "1       [come, lunch, togo, service, quick, staff, fri...\n",
              "2       [ive, vega, dozen, time, step, foot, circus, c...\n",
              "3       [night, close, street, party, actually, group,...\n",
              "4       [35, 4, starsnot, bad, price, 1299, lunch, sen...\n",
              "                              ...                        \n",
              "9995    [family, hungry, subway, open, 24, hour, guy, ...\n",
              "9996    [wife, come, couple, friend, sever, excite, po...\n",
              "9997    [food, okay, brag, food, hot, item, tasty, hor...\n",
              "9998    [today, visit, great, love, enjoy, town, squar...\n",
              "9999    [absolute, wrong, place, stay, 43, year, life,...\n",
              "Name: tokens, Length: 10000, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cfoKBuacinJi",
        "outputId": "1e421c45-296a-4672-c33e-268f954c93c3"
      },
      "source": [
        "df['tokens'][4] # example list of tokens"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['35',\n",
              " '4',\n",
              " 'starsnot',\n",
              " 'bad',\n",
              " 'price',\n",
              " '1299',\n",
              " 'lunch',\n",
              " 'senior',\n",
              " '15',\n",
              " 'pay',\n",
              " 'eat',\n",
              " 'hot',\n",
              " 'food',\n",
              " 'salad',\n",
              " 'noodle',\n",
              " 'bar',\n",
              " 'dessert',\n",
              " 'fruit',\n",
              " 'fry',\n",
              " 'variety',\n",
              " 'soupseating',\n",
              " 'middle',\n",
              " 'section',\n",
              " 'bite',\n",
              " 'tight',\n",
              " 'booth',\n",
              " 'look',\n",
              " 'spaciousi',\n",
              " 'think',\n",
              " 'lunch',\n",
              " 'noodle',\n",
              " 'bar',\n",
              " 'variety',\n",
              " 'different',\n",
              " 'type',\n",
              " 'green',\n",
              " 'vermicelli',\n",
              " 'noodle',\n",
              " 'freeze',\n",
              " 'tofu',\n",
              " 'etc',\n",
              " 'tomato',\n",
              " 'laksa',\n",
              " 'broth',\n",
              " 'pretty',\n",
              " 'good']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ALhUBATjGJK"
      },
      "source": [
        "## NearestNeighbors model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j0lgFYsrjNbB"
      },
      "source": [
        "vects = [nlp(doc).vector for doc in df['text']] # create vectors from the text"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gyKg6MbmjNew",
        "outputId": "66420b79-d584-458a-e23f-2c294a1bbfe6"
      },
      "source": [
        "nn = NearestNeighbors(n_neighbors=10, algorithm='ball_tree') # set the nearest neighbors using balltree\n",
        "nn.fit(vects) # fit the nn on the vects"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "NearestNeighbors(algorithm='ball_tree', leaf_size=30, metric='minkowski',\n",
              "                 metric_params=None, n_jobs=None, n_neighbors=10, p=2,\n",
              "                 radius=1.0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TbWdfHlRjNiN"
      },
      "source": [
        "# create the fake review\n",
        "created_review = \"\"\"\n",
        "The indian food was magnificent! We will come back.\n",
        "\"\"\"\n",
        "created_review_vect = nlp(created_review).vector # create a vector for the review"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nrxWmpdGjNpP"
      },
      "source": [
        "most_similiar = nn.kneighbors([created_review_vect]) # use nn model on the created review"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qMx33dl3jevn",
        "outputId": "d98401d1-97ab-426b-b2e4-c0fa9eaf743f"
      },
      "source": [
        "yelp.iloc[most_similiar[1][0]]['text'] # look at the similar reviews text"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1580    we made a reservation via internet but got no ...\n",
              "1896    we tried this place for the first time on sund...\n",
              "7256    this is an uber cool event we bought our ticke...\n",
              "5456    this past sunday 72918 my family an i were in ...\n",
              "2810    the food here is excellent this is by far our ...\n",
              "1181    this place was insane 26 for an intense seafoo...\n",
              "8719    a huge shout out to the cafe325 recommend to a...\n",
              "1799    overused quotes throughout the nightomg this i...\n",
              "8859    absolutely the best greek food ive ever eaten ...\n",
              "7623    after dinner we went to go see wanted at storm...\n",
              "Name: text, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UhTjRMF8kVSR"
      },
      "source": [
        "## Star reviews"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PnHpec5UkY-D"
      },
      "source": [
        "vect = TfidfVectorizer(stop_words=STOP_WORDS) # set the vector with TfidfVectorizer\n",
        "rfc = RandomForestClassifier() # set the random forest classifier"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wjOtvvFHkZCx"
      },
      "source": [
        "### create the pipeline ###\n",
        "pipe = Pipeline([\n",
        "                 ('vect', vect), # vectorizer\n",
        "                 ('clf', rfc) # classifier           \n",
        "                ])"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FnzfUGWzkZJc",
        "outputId": "3a47eb15-ce06-4ac2-d5e3-5c3772fbbb18"
      },
      "source": [
        "# set the parameters\n",
        "parameters = {\n",
        "    'vect__max_df': ( 0.5, 0.75, 1.0, 1.25, 1.50),\n",
        "    'vect__min_df': (.01, .03, .05, .07, .09)\n",
        "    }  \n",
        "grid_search = GridSearchCV(pipe, parameters, cv=5, n_jobs=-1, verbose=1) # use GRidSearchCV on the pipe and parameters\n",
        "grid_search.fit(df['text'], df['stars']) # fit the data on the grid_search"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 25 candidates, totalling 125 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  46 tasks      | elapsed:  5.3min\n",
            "[Parallel(n_jobs=-1)]: Done 125 out of 125 | elapsed: 14.2min finished\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ll', 've'] not in stop_words.\n",
            "  'stop_words.' % sorted(inconsistent))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GridSearchCV(cv=5, error_score=nan,\n",
              "             estimator=Pipeline(memory=None,\n",
              "                                steps=[('vect',\n",
              "                                        TfidfVectorizer(analyzer='word',\n",
              "                                                        binary=False,\n",
              "                                                        decode_error='strict',\n",
              "                                                        dtype=<class 'numpy.float64'>,\n",
              "                                                        encoding='utf-8',\n",
              "                                                        input='content',\n",
              "                                                        lowercase=True,\n",
              "                                                        max_df=1.0,\n",
              "                                                        max_features=None,\n",
              "                                                        min_df=1,\n",
              "                                                        ngram_range=(1, 1),\n",
              "                                                        norm='l2',\n",
              "                                                        preprocessor=None,\n",
              "                                                        smooth_idf=True,\n",
              "                                                        stop_words={\"'d\", \"'ll\",\n",
              "                                                                    \"'m...\n",
              "                                                               min_samples_split=2,\n",
              "                                                               min_weight_fraction_leaf=0.0,\n",
              "                                                               n_estimators=100,\n",
              "                                                               n_jobs=None,\n",
              "                                                               oob_score=False,\n",
              "                                                               random_state=None,\n",
              "                                                               verbose=0,\n",
              "                                                               warm_start=False))],\n",
              "                                verbose=False),\n",
              "             iid='deprecated', n_jobs=-1,\n",
              "             param_grid={'vect__max_df': (0.5, 0.75, 1.0, 1.25, 1.5),\n",
              "                         'vect__min_df': (0.01, 0.03, 0.05, 0.07, 0.09)},\n",
              "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
              "             scoring=None, verbose=1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lU-SPBCvkZN-",
        "outputId": "9f8a430b-5501-47fa-c259-4b5b373faba1"
      },
      "source": [
        "grid_search.best_score_ # show the grid search best score "
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.5827"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 78
        },
        "id": "3IRFYBguki1j",
        "outputId": "81117390-0264-49f7-bc9f-90622a364307"
      },
      "source": [
        "created_review = [created_review] # set the review to a list \n",
        "pred = grid_search.predict(created_review) # use grid search predict on the review \n",
        "created_review_stars = pd.DataFrame({'text': created_review, 'stars':pred}) # craete dataframe with reivew and stars\n",
        "created_review_stars['stars'] = created_review_stars['stars'].astype('int64') # change the data type to int\n",
        "created_review_stars['text'] = created_review_stars.text.replace('\\n':'')\n",
        "pd.set_option('display.max_colwidth',1000)\n",
        "created_review_stars.head()"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>stars</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>\\nThe indian food was magnificent! We will come back.\\n</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                      text  stars\n",
              "0  \\nThe indian food was magnificent! We will come back.\\n      5"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    }
  ]
}